{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "This notebook trains a Transformer-based image captioning model with a multi-phase training approach using CLIP score optimization. This notebook contains the same code as the seperately uploaded project, with alterations to allow it to run on Google Colab. The model learns to generate descriptive captions for images through a combination of cross-entropy loss and CLIP-based policy gradient loss. I reduced the amount of images provided in the folder to be placed in the Google Drive to 10K images instead of the full 140K images I used. This is to reduce the size of the files to be handed in, while still allowing this notebook to be tested. The results of training on this small subset of the larger dataset is not representative of the results achieved on the full 140K image-caption pairs. The provided dataset serves merely to showcase the functionality of the code.\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "### Prerequisites\n",
        "- Google Colab with GPU runtime enabled\n",
        "- Google Drive for storing datasets and models\n",
        "\n",
        "### First-Time Setup\n",
        "\n",
        "1. **Upload the provided zip file** to your Google Drive.\n",
        "   - The zip file contains necessary images and caption data\n",
        "   - Place it in the path specified in the notebook (default: `/content/drive/MyDrive/ImageCaptionColab/)\n",
        "   - Unzip so that your drive contains the unzipped folder called ImageCaptionColab\n",
        "\n",
        "2. **Run the empty/broken image finder** (third codeblock of the notebook) before running the main pipeline\n",
        "   - This will identify and remove any corrupt images that might cause training issues\n",
        "   - Adjust the image directory paths in this cell if needed\n",
        "\n",
        "3. **Check configuration variables** at the beginning of the notebook:\n",
        "   - `BASE_PATH`: Base directory for all files\n",
        "   - `METRICS_PATH`: Path to metric calculation files\n",
        "   - `ZIP_PATH`: Path to the zipped images\n",
        "   - `IMAGE_FOLDER`: Where extracted images will be stored\n",
        "   - `CAPTIONS_FILE`: Path to captions TSV file\n",
        "   - `MODEL_PATH`: Path for loading pre-trained model (if any)\n",
        "   - `OUTPUT_PATH`: Where to save the fine-tuned model\n",
        "\n",
        "### Running the Training Pipeline\n",
        "\n",
        "After placing the correct folder in the Google Drive you can run the cells in order from top to bottom:\n",
        "\n",
        "1. **Copy metric files** and set up the environment\n",
        "   - The notebook will copy necessary metric calculation files\n",
        "   - Required Python packages will be installed\n",
        "\n",
        "2. **Extract images from zip**\n",
        "   - Images will be extracted to the specified folder\n",
        "   - By default, it extracts up to 10,000 images for training\n",
        "\n",
        "3. **Feature extraction and data preparation**\n",
        "   - Image features are extracted using ResNet18\n",
        "   - Features are cached to disk to speed up future runs\n",
        "   - Vocabulary is built from captions\n",
        "\n",
        "4. **Multi-phase training**\n",
        "   - **Phase 1**: Training with cross-entropy loss only\n",
        "   - **Phase 2**: Combined cross-entropy and CLIP-based loss\n",
        "   - **Phase 3**: Fine-tuning with high CLIP loss weight\n",
        "\n",
        "5. **Evaluation**\n",
        "   - The model is evaluated using CLIP score\n",
        "   - Training curves and sample captions are provided\n",
        "\n",
        "## Training Parameters\n",
        "\n",
        "The training process can be customized by modifying the `base_config` dictionary:\n",
        "\n",
        "- `max_images`: Maximum number of images to use (default: 10,000)\n",
        "- `batch_size`: Batch size for training (default: 64)\n",
        "- `embed_size`, `hidden_size`, `num_layers`: Model architecture parameters\n",
        "- `learning_rate`: Initial learning rate\n",
        "- `num_epochs`: Number of epochs for each training phase\n",
        "- `clip_loss_weight`: Weight for the CLIP-based loss component\n",
        "\n",
        "## Output\n",
        "\n",
        "After training completes, the following files are generated:\n",
        "\n",
        "- Saved models for each training phase\n",
        "- The best model based on validation loss and CLIP score\n",
        "- Training curves showing loss and CLIP score progression\n",
        "- Evaluation metrics and sample generated captions"
      ],
      "metadata": {
        "id": "zOZWqBUyk2bD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qq4REUfTkxc-"
      },
      "outputs": [],
      "source": [
        "# Part 0: Copy metric files to ensure they can be imported\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Paths\n",
        "BASE_PATH = \"/content/drive/MyDrive\"  # Update this path if needed\n",
        "METRICS_PATH = f\"{BASE_PATH}/ImageCaptionColab/TrainMetric\"  # folder containing the metric py files\n",
        "SOURCE_DIR = f\"{BASE_PATH}/ImageCaptionColab/TrainMetric\"\n",
        "DEST_DIR = \"/content\"  # current working directory\n",
        "\n",
        "# list of metric files to copy (if they're not in a metrics folder)\n",
        "metric_files = [\n",
        "    \"base.py\",\n",
        "    \"clip_scorer.py\",\n",
        "    \"grammar_checker.py\",\n",
        "    \"clip.py\",\n",
        "    \"metric.py\"\n",
        "]\n",
        "\n",
        "# copy each file to current directory\n",
        "print(\"copying metric files to current directory...\")\n",
        "for filename in metric_files:\n",
        "    source_path = os.path.join(SOURCE_DIR, filename)\n",
        "    dest_path = os.path.join(DEST_DIR, filename)\n",
        "\n",
        "    if os.path.exists(source_path):\n",
        "        shutil.copy2(source_path, dest_path)\n",
        "        print(f\"  Copied {filename} ✓\")\n",
        "    else:\n",
        "        print(f\"  {filename} not found in source directory ✗\")\n",
        "\n",
        "!pip install language_tool_python transformers Pillow torch torchvision matplotlib pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# part 1: setup and import metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from collections import Counter\n",
        "import math\n",
        "import sys\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# change the path to the metrics folder,\n",
        "sys.path.append(METRICS_PATH)  # if they're in a folder in my google drive\n",
        "\n",
        "# check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# paths for the project\n",
        "CAPTIONS_FILE = f\"{BASE_PATH}/ImageCaptionColab/filtered_captions.tsv\" # all captions for images that could be extracted (removed those for broken urls)\n",
        "MODEL_PATH = f\"{BASE_PATH}/best_model.pt\"  # path to pre-trained model\n",
        "OUTPUT_PATH = f\"{BASE_PATH}/finetuned_model.pt\"  # where to save fine-tuned model\n",
        "FILTERED_CAPTIONS_FILE = f\"{BASE_PATH}/filtered_captions_matched.tsv\"\n",
        "\n",
        "\n",
        "# Define your paths\n",
        "BASE_PATH = \"/content/drive/MyDrive\"  # Update to your actual base path\n",
        "ZIP_PATH = f\"{BASE_PATH}/ImageCaptionColab/loadedimages.zip\"\n",
        "IMAGE_FOLDER = f\"{BASE_PATH}/ImageCaptionColab/images/images\"  # final destination folder\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(IMAGE_FOLDER, exist_ok=True)\n",
        "\n",
        "# Function to extract and flatten the directory structure\n",
        "def extract_images(zip_path, dest_folder, max_images=10094):\n",
        "    \"\"\"\n",
        "    extract images from ZIP file and place them directly in the destination folder,\n",
        "    ignoring any folder structure in the ZIP\n",
        "    \"\"\"\n",
        "    print(f\"Extracting up to {max_images} images from {zip_path} to {dest_folder}...\")\n",
        "\n",
        "    # make temporary extraction folder\n",
        "    temp_folder = os.path.join(os.path.dirname(dest_folder), \"temp_extract\")\n",
        "    os.makedirs(temp_folder, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # Get list of image files in the zip\n",
        "        all_files = [f for f in zip_ref.namelist() if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        # Limit to max_images\n",
        "        if len(all_files) > max_images:\n",
        "            print(f\"ZIP contains {len(all_files)} images, limiting extraction to {max_images}\")\n",
        "            # Shuffle the list to get a random sample\n",
        "            random.seed(42)  # For reproducibility\n",
        "            random.shuffle(all_files)\n",
        "            file_list = all_files[:max_images]\n",
        "        else:\n",
        "            file_list = all_files\n",
        "\n",
        "        # Extract the selected images to temp folder\n",
        "        for file in tqdm(file_list, desc=\"extracting to temp folder\"):\n",
        "            zip_ref.extract(file, temp_folder)\n",
        "\n",
        "        # move files from temp folder to destination, flattening directory structure\n",
        "        # quick fix for an issue I ran into with images being placed in a folder in the folder\n",
        "        count = 0\n",
        "        skipped = 0\n",
        "        for root, _, files in os.walk(temp_folder):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    # Get original filename without path\n",
        "                    src_path = os.path.join(root, file)\n",
        "                    # Use just the base filename for destination\n",
        "                    dest_path = os.path.join(dest_folder, file)\n",
        "\n",
        "                    # If file already exists, skip it\n",
        "                    if os.path.exists(dest_path):\n",
        "                        skipped += 1\n",
        "                        continue\n",
        "\n",
        "                    shutil.copy2(src_path, dest_path)\n",
        "                    count += 1\n",
        "\n",
        "        # remove temporary folder\n",
        "        shutil.rmtree(temp_folder)\n",
        "\n",
        "        print(f\"extracted and flattened {count} images to {dest_folder}\")\n",
        "        return count\n",
        "\n",
        "# Check if extraction is needed\n",
        "if len([f for f in os.listdir(IMAGE_FOLDER) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) < 10000:\n",
        "    # clear the destination folder first to avoid mixed content\n",
        "    for file in os.listdir(IMAGE_FOLDER):\n",
        "        file_path = os.path.join(IMAGE_FOLDER, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            os.remove(file_path)\n",
        "\n",
        "\n",
        "    extract_images(ZIP_PATH, IMAGE_FOLDER, MAX_EXTRACT_IMAGES)\n",
        "else:\n",
        "    print(f\"images already extracted in {IMAGE_FOLDER}\")\n",
        "    print(f\"found {len([f for f in os.listdir(IMAGE_FOLDER) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])} image files\")\n",
        "\n",
        "\n",
        "def filter_captions_file(captions_file, image_folder, output_file=None):\n",
        "    \"\"\"\n",
        "    make sure the captions file only has entries for images that actually exist\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "    print(f\"getting captions from: {captions_file}\")\n",
        "    # read the captions file\n",
        "    df = pd.read_csv(captions_file, sep='\\t', header=None, names=['image_id', 'caption'])\n",
        "    original_count = len(df)\n",
        "    print(f\"original captions count: {original_count}\")\n",
        "\n",
        "    # list of available images (directly in the image folder, no nested structure this time grr)\n",
        "    print(f\"checking available images in: {image_folder}\")\n",
        "    available_images = set()\n",
        "    for file in os.listdir(image_folder):\n",
        "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            # both with and without extension\n",
        "            base_name = os.path.splitext(file)[0]\n",
        "            available_images.add(base_name)\n",
        "            available_images.add(file)\n",
        "\n",
        "    print(f\"found {len(available_images)} available images\")\n",
        "\n",
        "    # filter captions to only include those for available images\n",
        "    print(\"filtering captions...\")\n",
        "    filtered_df = df[df['image_id'].astype(str).isin(available_images)]\n",
        "    filtered_count = len(filtered_df)\n",
        "    print(f\"nr of filtered captions: {filtered_count}\")\n",
        "    print(f\"removed {original_count - filtered_count} captions for missing images\")\n",
        "\n",
        "    if output_file:\n",
        "        print(f\"saving filtered captions to: {output_file}...\")\n",
        "        filtered_df.to_csv(output_file, sep='\\t', header=False, index=False)\n",
        "        print(f\"saved filtered captions file with {filtered_count} entries!\")\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "# alternative version that combines train and val datasets\n",
        "def filter_to_all_training_images(reference_captions, generated_captions, image_paths,\n",
        "                                train_loader, val_loader):\n",
        "    \"\"\"\n",
        "    filter evaluation images to match exactly those used in both training and validation\n",
        "    \"\"\"\n",
        "    # extract image IDs from both datasets\n",
        "    train_image_ids = set(train_loader.dataset.image_names)\n",
        "    val_image_ids = set(val_loader.dataset.image_names)\n",
        "    all_used_image_ids = train_image_ids.union(val_image_ids)\n",
        "\n",
        "    print(f\"found {len(train_image_ids)} training and {len(val_image_ids)} validation images\")\n",
        "    print(f\"total of {len(all_used_image_ids)} unique images used in training/validation\")\n",
        "\n",
        "    # filter dictionaries to only include these IDs\n",
        "    filtered_reference = {img_id: captions for img_id, captions in reference_captions.items()\n",
        "                         if img_id in all_used_image_ids}\n",
        "    filtered_generated = {img_id: caption for img_id, caption in generated_captions.items()\n",
        "                         if img_id in all_used_image_ids}\n",
        "    filtered_paths = {img_id: path for img_id, path in image_paths.items()\n",
        "                     if img_id in all_used_image_ids}\n",
        "\n",
        "    print(f\"filtered to {len(filtered_reference)} reference images\")\n",
        "    print(f\"filtered to {len(filtered_generated)} generated images\")\n",
        "    print(f\"filtered to {len(filtered_paths)} image paths\")\n",
        "\n",
        "    return filtered_reference, filtered_generated, filtered_paths"
      ],
      "metadata": {
        "id": "pjmLZQIMlTrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CAN SKIP THIS CELL WHEN NO NEW IMAGES WERE EXTRACTED FROM\n",
        "# loadedimages.zip\n",
        "\n",
        "\n",
        "# empty/broken image finder\n",
        "# minimal checks, focuses on file size\n",
        "# use once at the beginning (after extracting the desired amount of images from the zip file)\n",
        "# to ensure all images actually exist\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import concurrent.futures\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# try to mount drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully!\")\n",
        "except:\n",
        "    print(\"drive already mounted or could not be mounted\")\n",
        "\n",
        "# Configuration - ADJUST THESE\n",
        "IMAGE_DIR = f\"{BASE_PATH}/ImageCaptionColab/images/images\"  # image directory\n",
        "OUTPUT_DIR = f\"{BASE_PATH}/ImageCaptionColab/broken_images\"  # where to move broken images\n",
        "EXTENSIONS = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'] # really only have one of them, but you never know\n",
        "MIN_SIZE_BYTES = 1000  # any file smaller than this is considered empty/broken\n",
        "MAX_WORKERS = 4  # can be higher since we're doing very simple checks\n",
        "BATCH_SIZE = 100  # can be much larger since we're not loading images\n",
        "DELETE_BROKEN = False  # set to True to delete broken images instead of moving them\n",
        "\n",
        "# function to check if a file is empty or too small\n",
        "def is_broken_file(file_path, min_size=MIN_SIZE_BYTES):\n",
        "    \"\"\"\n",
        "    check if a file is too small to be a valid image\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # get file size\n",
        "        size = os.path.getsize(file_path)\n",
        "        if size < min_size:\n",
        "            return True, f\"file too small: {size} bytes\"\n",
        "\n",
        "        # optional quick header check for slightly larger files\n",
        "        # I don't think this code was ever actually needed\n",
        "        if size < 5000:  # only check headers for suspicious small files\n",
        "            try:\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    header = f.read(16)\n",
        "\n",
        "                # common image file headers\n",
        "                valid_headers = [\n",
        "                    b'\\xff\\xd8\\xff',  # JPEG\n",
        "                    b'\\x89PNG\\r\\n\\x1a\\n',  # PNG\n",
        "                    b'GIF8',  # GIF\n",
        "                    b'BM',  # BMP\n",
        "                    b'RIFF'  # WEBP\n",
        "                ]\n",
        "\n",
        "                # check if header matches any valid image header\n",
        "                if not any(header.startswith(h) for h in valid_headers):\n",
        "                    return True, \"invalid image header\"\n",
        "            except:\n",
        "                # if we can't read the file, consider it broken\n",
        "                return True, \"can't read file\"\n",
        "\n",
        "        return False, \"OK\"\n",
        "    except Exception as e:\n",
        "        return True, str(e)\n",
        "\n",
        "# function to process a batch of files\n",
        "def process_batch(files):\n",
        "    \"\"\"\n",
        "    process a batch of files, return list of broken files\n",
        "    \"\"\"\n",
        "    broken = []\n",
        "    for file_path in files:\n",
        "        is_broken, reason = is_broken_file(file_path)\n",
        "        if is_broken:\n",
        "            broken.append((file_path, reason))\n",
        "    return broken\n",
        "\n",
        "# main function to find and remove empty/broken images\n",
        "def find_and_remove_empty_files():\n",
        "    \"\"\"\n",
        "    find and remove empty or broken image files\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # make sure output directory exists if we're moving files\n",
        "    if not DELETE_BROKEN and not os.path.exists(OUTPUT_DIR):\n",
        "        os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "    # get all image files\n",
        "    print(f\"scanning directory: {IMAGE_DIR}\")\n",
        "    image_files = []\n",
        "\n",
        "    for root, _, files in os.walk(IMAGE_DIR):\n",
        "        for file in files:\n",
        "            _, ext = os.path.splitext(file)\n",
        "            if ext.lower() in EXTENSIONS:\n",
        "                image_files.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"found {len(image_files)} image files to check\")\n",
        "\n",
        "    # split into batches for processing\n",
        "    batches = [image_files[i:i+BATCH_SIZE] for i in range(0, len(image_files), BATCH_SIZE)]\n",
        "\n",
        "    # process all batches\n",
        "    all_broken_files = []\n",
        "    files_processed = 0\n",
        "\n",
        "    print(f\"processing in {len(batches)} batches with {MAX_WORKERS} workers\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(batches):\n",
        "        batch_start = time.time()\n",
        "\n",
        "        # process the batch\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "            # split into smaller sub-batches for each worker\n",
        "            sub_batch_size = max(1, len(batch) // MAX_WORKERS)\n",
        "            sub_batches = [batch[i:i+sub_batch_size] for i in range(0, len(batch), sub_batch_size)]\n",
        "\n",
        "            # process sub-batches in parallel\n",
        "            futures = [executor.submit(process_batch, sub_batch) for sub_batch in sub_batches]\n",
        "            broken_batches = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
        "\n",
        "            # flatten results\n",
        "            batch_broken = [item for sublist in broken_batches for item in sublist]\n",
        "\n",
        "        # handle broken files\n",
        "        for file_path, reason in batch_broken:\n",
        "            all_broken_files.append((file_path, reason))\n",
        "\n",
        "            # remove or move the file\n",
        "            if DELETE_BROKEN:\n",
        "                try:\n",
        "                    os.remove(file_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error deleting {file_path}: {e}\")\n",
        "            else:\n",
        "                try:\n",
        "                    filename = os.path.basename(file_path)\n",
        "                    dest_path = os.path.join(OUTPUT_DIR, filename)\n",
        "\n",
        "                    # handle filename problems\n",
        "                    if os.path.exists(dest_path):\n",
        "                        base, ext = os.path.splitext(filename)\n",
        "                        dest_path = os.path.join(OUTPUT_DIR, f\"{base}_{int(time.time()*1000) % 10000}{ext}\")\n",
        "\n",
        "                    shutil.move(file_path, dest_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error moving {file_path}: {e}\")\n",
        "\n",
        "        # update progress\n",
        "        files_processed += len(batch)\n",
        "        progress_percent = files_processed / len(image_files) * 100\n",
        "\n",
        "        # calculate timing metrics\n",
        "        batch_time = time.time() - batch_start\n",
        "        files_per_second = len(batch) / max(0.001, batch_time)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        # estimated time remaining\n",
        "        remaining_files = len(image_files) - files_processed\n",
        "        if files_per_second > 0:\n",
        "            estimated_time_remaining = remaining_files / files_per_second\n",
        "        else:\n",
        "            estimated_time_remaining = 0\n",
        "\n",
        "        # print progress\n",
        "        print(f\"batch {batch_idx+1}/{len(batches)} | \"\n",
        "              f\"progress: {progress_percent:.1f}% | \"\n",
        "              f\"found {len(batch_broken)} broken files in this batch | \"\n",
        "              f\"speed: {files_per_second:.1f} files/sec\")\n",
        "\n",
        "    # make a little report\n",
        "    report_path = os.path.join(OUTPUT_DIR if not DELETE_BROKEN else os.path.dirname(IMAGE_DIR),\n",
        "                               \"empty_files_report.txt\")\n",
        "\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(\"# empty/broken files report\\n\\n\")\n",
        "        f.write(f\"total files processed: {len(image_files)}\\n\")\n",
        "        f.write(f\"empty/broken files found: {len(all_broken_files)}\\n\\n\")\n",
        "        f.write(\"## list of empty/broken files\\n\\n\")\n",
        "\n",
        "        for file_path, reason in all_broken_files:\n",
        "            f.write(f\"{file_path}\\t{reason}\\n\")\n",
        "\n",
        "    # Final stats\n",
        "    total_time = time.time() - start_time\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PROCESSING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"processed {len(image_files)} files in {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")\n",
        "    print(f\"found {len(all_broken_files)} empty or broken files\")\n",
        "    print(f\"average speed: {len(image_files)/total_time:.1f} files per second\")\n",
        "    print(f\"report saved to: {report_path}\")\n",
        "\n",
        "    return all_broken_files\n",
        "\n",
        "# execute the function\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"starting empty/broken file finder...\")\n",
        "    broken_files = find_and_remove_empty_files()\n",
        "    print(\"done! :)\")"
      ],
      "metadata": {
        "id": "gATrfxhYxZiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# part 2: model definition and datasets\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from collections import Counter\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import math\n",
        "import language_tool_python\n",
        "\n",
        "torch.serialization.add_safe_globals([\n",
        "    np.core.multiarray.scalar,\n",
        "    np.dtype,\n",
        "    np.ndarray,\n",
        "    np._globals\n",
        "])\n",
        "\n",
        "# check if GPU is available (again i guess)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"using: {device}\")\n",
        "\n",
        "class CaptionFeatureDataset(Dataset):\n",
        "    def __init__(self, features_dict, captions_dict, word2idx, max_len=22, split='train'):\n",
        "        \"\"\"\n",
        "        dataset for training/validation (now with extra safety checks).\n",
        "\n",
        "        Args:\n",
        "            features_dict: dictionary mapping image_id to image features\n",
        "            captions_dict: dictionary mapping image_id to list of captions\n",
        "            word2idx: Word to index mapping\n",
        "            max_len: max caption length\n",
        "            split: 'train' or 'val'\n",
        "        \"\"\"\n",
        "        # first, ensure data consistency - only use images with both features and captions (i'm finally learning from my mistakes)\n",
        "        common_ids = set(features_dict.keys()).intersection(set(captions_dict.keys()))\n",
        "        print(f\"Dataset '{split}': {len(common_ids)} valid images (from {len(features_dict)} features, {len(captions_dict)} caption sets)\")\n",
        "\n",
        "        if len(common_ids) == 0:\n",
        "            raise ValueError(\"no valid images found with both features and captions! :(\")\n",
        "\n",
        "        # get all valid image IDs\n",
        "        all_ids = list(common_ids)\n",
        "        random.seed(42)  # for reproducibility\n",
        "        random.shuffle(all_ids)\n",
        "\n",
        "        # split into train/val (90%/10%), probably should have made this an argument as well\n",
        "        if split == 'train':\n",
        "            self.image_names = all_ids[:int(0.9 * len(all_ids))]\n",
        "        else:\n",
        "            self.image_names = all_ids[int(0.9 * len(all_ids)):]\n",
        "\n",
        "        self.features_dict = features_dict\n",
        "        self.captions_dict = captions_dict\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "        print(f\"created {split} dataset with {len(self.image_names)} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_names[idx]\n",
        "\n",
        "        # double safety check (should never fail due to the initialization, but i'm paranoid now)\n",
        "        if img_name not in self.features_dict:\n",
        "            raise KeyError(f\"image {img_name} not found in features dictionary!\")\n",
        "        if img_name not in self.captions_dict:\n",
        "            raise KeyError(f\"image {img_name} not found in captions dictionary!\")\n",
        "\n",
        "        feature = self.features_dict[img_name]\n",
        "\n",
        "        # for training, randomly choose one caption\n",
        "        caption = random.choice(self.captions_dict[img_name]).split()\n",
        "\n",
        "        # add < SOS > and <EOS> tokens\n",
        "        caption = [\"< SOS >\"] + caption + [\"<EOS>\"]\n",
        "        tokens = [self.word2idx.get(w, self.word2idx[\"<UNK>\"]) for w in caption]\n",
        "        tokens = tokens[:self.max_len]\n",
        "        tokens += [self.word2idx[\"<PAD>\"]] * (self.max_len - len(tokens))\n",
        "\n",
        "        return feature, torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "\n",
        "class CaptionEvaluationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    dataset for evaluation with safety checks\n",
        "    \"\"\"\n",
        "    def __init__(self, features_dict):\n",
        "        self.image_names = list(features_dict.keys())\n",
        "        self.features_dict = features_dict\n",
        "        print(f\"created evaluation dataset with {len(self.image_names)} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_names[idx]\n",
        "        feature = self.features_dict[img_name]\n",
        "        return feature, img_name\n",
        "\n",
        "# using Mirha Sidheek's Transformer Decoder model implementation\n",
        "class Transformer_Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_encoding = nn.Parameter(torch.zeros(1, 22, embed_size))  # max_len=22\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=embed_size,\n",
        "            nhead=16,\n",
        "            dim_feedforward=hidden_size,\n",
        "            dropout=0.2,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions, mask=None):\n",
        "        batch_size = captions.size(0)\n",
        "        seq_len = captions.size(1)\n",
        "\n",
        "        # Embed captions and add positional encoding\n",
        "        embedded = self.embedding(captions) + self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        # Create causal mask for autoregressive decoding if not provided\n",
        "        if mask is None:\n",
        "            mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(features.device)\n",
        "\n",
        "        # Features as memory (B, 1, embed_size) for cross-attention\n",
        "        memory = features.unsqueeze(1)  # Add sequence dimension\n",
        "\n",
        "        # Decode\n",
        "        output = self.decoder(tgt=embedded, memory=memory, tgt_mask=mask)\n",
        "        return self.fc(output)\n",
        "\n",
        "# Beam search caption generation\n",
        "def beam_search_caption(image_feature, decoder, project_features, word2idx, idx2word, device,\n",
        "                        beam_width=5, max_len=22):\n",
        "    \"\"\"\n",
        "    Generate a caption using beam search.\n",
        "\n",
        "    Args:\n",
        "        image_feature: Image feature tensor\n",
        "        decoder: The decoder model\n",
        "        project_features: The projection layer\n",
        "        word2idx: Word to index mapping\n",
        "        idx2word: Index to word mapping\n",
        "        device: Device to run on\n",
        "        beam_width: Beam width\n",
        "        max_len: Maximum caption length\n",
        "\n",
        "    Returns:\n",
        "        The generated caption\n",
        "    \"\"\"\n",
        "    decoder.eval()\n",
        "    project_features.eval()\n",
        "\n",
        "    # Project the image features\n",
        "    with torch.no_grad():\n",
        "        projected = project_features(image_feature)\n",
        "\n",
        "    # Beam search\n",
        "    sequences = [[[word2idx[\"< SOS >\"]], 0.0]]  # (sequence, log_prob)\n",
        "    completed = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        candidates = []\n",
        "        for seq, score in sequences:\n",
        "            if seq[-1] == word2idx[\"<EOS>\"]:\n",
        "                completed.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            input_seq = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = decoder(projected, input_seq)\n",
        "                probs = torch.softmax(output[:, -1, :], dim=-1)\n",
        "                topk = torch.topk(probs, beam_width)\n",
        "\n",
        "            for i in range(beam_width):\n",
        "                token = topk.indices[0, i].item()\n",
        "                token_prob = topk.values[0, i].item()\n",
        "                new_seq = seq + [token]\n",
        "                new_score = score + torch.log(torch.tensor(token_prob + 1e-10)).item()\n",
        "                candidates.append((new_seq, new_score))\n",
        "\n",
        "        # Select top beam_width candidates\n",
        "        sequences = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "        if len(sequences) == 0:\n",
        "            break\n",
        "\n",
        "    # Include completed sequences\n",
        "    all_sequences = sequences + completed\n",
        "    if not all_sequences:\n",
        "        return \"\"\n",
        "\n",
        "    best_seq = sorted(all_sequences, key=lambda x: x[1], reverse=True)[0][0]\n",
        "\n",
        "    # Convert to words and return caption\n",
        "    caption_tokens = [idx2word[t] for t in best_seq if t not in [word2idx[\"<PAD>\"], word2idx[\"< SOS >\"], word2idx[\"<EOS>\"]]]\n",
        "    return \" \".join(caption_tokens)\n",
        "\n",
        "def sample_caption(image_feature, decoder, project_features, word2idx, idx2word, device,\n",
        "                   max_len=22, temperature=1.0):\n",
        "    \"\"\"\n",
        "    sample a caption using the current policy (the decoder), keeping track of log probabilities\n",
        "    for policy gradient training\n",
        "\n",
        "    Args:\n",
        "        image_feature: image feature tensor\n",
        "        decoder: decoder model\n",
        "        project_features: projection layer\n",
        "        word2idx: word to index mapping\n",
        "        idx2word: index to word mapping\n",
        "        device: device to run on\n",
        "        max_len: max caption length\n",
        "        temperature: temperature for sampling (higher = more diverse)\n",
        "\n",
        "    Returns:\n",
        "        tokens: list of token ids for the sampled caption\n",
        "        log_probs: sum of log probabilities for the sampled tokens\n",
        "        caption: caption\n",
        "    \"\"\"\n",
        "    decoder.eval()  # set to eval mode initially\n",
        "    project_features.eval()\n",
        "\n",
        "    # project image features\n",
        "    with torch.no_grad():\n",
        "        projected = project_features(image_feature.unsqueeze(0))\n",
        "\n",
        "    # switch to train mode for sampling\n",
        "    decoder.train()\n",
        "\n",
        "    # start with SOS token\n",
        "    tokens = [word2idx[\"< SOS >\"]]\n",
        "    log_probs_list = []\n",
        "\n",
        "    # generate tokens one by one\n",
        "    for i in range(max_len - 1):  # -1 because we've already added SOS\n",
        "        # prepare current sequence\n",
        "        curr_seq = torch.tensor([tokens], dtype=torch.long).to(device)\n",
        "\n",
        "        # get predictions\n",
        "        with torch.enable_grad():  # make sure we're tracking gradients\n",
        "            outputs = decoder(projected, curr_seq)\n",
        "            logits = outputs[:, -1, :] / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # sample from the distribution\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            next_token = dist.sample()\n",
        "            log_prob = dist.log_prob(next_token)\n",
        "\n",
        "            # add to tracking variables\n",
        "            log_probs_list.append(log_prob)\n",
        "            tokens.append(next_token.item())\n",
        "\n",
        "            # stop if we encounter the EOS token\n",
        "            if next_token.item() == word2idx[\"<EOS>\"]:\n",
        "                break\n",
        "\n",
        "    # convert token IDs to words (excluding SOS, EOS, and PAD)\n",
        "    caption_words = [idx2word[token] for token in tokens\n",
        "                    if token not in [word2idx[\"<PAD>\"], word2idx[\"< SOS >\"], word2idx[\"<EOS>\"]]]\n",
        "    caption = \" \".join(caption_words)\n",
        "\n",
        "    # return the tokens, sum of log probabilities, and the caption text\n",
        "    return tokens, torch.stack(log_probs_list).sum(), caption\n"
      ],
      "metadata": {
        "id": "1815plnXmEN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# part 3: save and load model\n",
        "\n",
        "def save_model(decoder, project_features, metrics, output_path, model_type=\"checkpoint\"):\n",
        "    \"\"\"\n",
        "    save model with comprehensive metadata\n",
        "\n",
        "    Args:\n",
        "        decoder: decoder model\n",
        "        project_features: features projection layer\n",
        "        metrics: dictionary of metrics to save with the model\n",
        "        output_path: base path for saving\n",
        "        model_type: type of model being saved (checkpoint, best_val, best_clip)\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import torch\n",
        "    import time\n",
        "    from datetime import datetime\n",
        "\n",
        "    # create a timestamped filename so i finally keep track properly\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    if model_type == \"checkpoint\":\n",
        "        filename = f\"{output_path}_{model_type}_{timestamp}.pt\"\n",
        "    else:\n",
        "        filename = f\"{output_path}_{model_type}.pt\"\n",
        "\n",
        "    # prepare model save dictionary with metadata\n",
        "    save_dict = {\n",
        "        'decoder': decoder.state_dict(),\n",
        "        'project_features': project_features.state_dict(),\n",
        "        'metrics': metrics,\n",
        "        'timestamp': timestamp,\n",
        "        'type': model_type\n",
        "    }\n",
        "\n",
        "    # save model\n",
        "    print(f\"Saving {model_type} model to {filename}...\")\n",
        "    torch.save(save_dict, filename)\n",
        "\n",
        "    # if this is a best model, also save a copy to the standard path, kinda wonky, but works for now\n",
        "    if model_type in [\"best_val\", \"best_clip\"]:\n",
        "        standard_path = f\"{output_path}.pt\"\n",
        "        print(f\"Also saving to standard path: {standard_path}\")\n",
        "        torch.save(save_dict, standard_path)\n",
        "\n",
        "    return filename\n",
        "\n",
        "def load_model(model_path, embed_size, vocab_size, hidden_size, num_layers, device):\n",
        "    \"\"\"\n",
        "    load a saved model (now with proper error handling)\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import torch\n",
        "\n",
        "    print(f\"Loading model from {model_path}...\")\n",
        "\n",
        "    try:\n",
        "        # try loading the model with weights_only=False\n",
        "        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
        "\n",
        "        # initialize the models\n",
        "        decoder = Transformer_Decoder(embed_size, vocab_size, hidden_size, num_layers).to(device)\n",
        "        project_features = nn.Linear(512, embed_size).to(device)\n",
        "\n",
        "        # load the weights\n",
        "        decoder.load_state_dict(checkpoint['decoder'])\n",
        "        project_features.load_state_dict(checkpoint['project_features'])\n",
        "\n",
        "        # extract metadata if available\n",
        "        metadata = {}\n",
        "        for key in ['metrics', 'timestamp', 'type']:\n",
        "            if key in checkpoint:\n",
        "                metadata[key] = checkpoint[key]\n",
        "\n",
        "        print(f\"model loaded successfully!\")\n",
        "        if 'type' in metadata:\n",
        "            print(f\"Model type: {metadata['type']}\")\n",
        "        if 'timestamp' in metadata:\n",
        "            print(f\"Saved on: {metadata['timestamp']}\")\n",
        "\n",
        "        return decoder, project_features, metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Initializing new model instead...\")\n",
        "\n",
        "        # initialize new models if there was none found\n",
        "        decoder = Transformer_Decoder(embed_size, vocab_size, hidden_size, num_layers).to(device)\n",
        "        project_features = nn.Linear(2048, embed_size).to(device)\n",
        "\n",
        "        return decoder, project_features, {}\n",
        "\n",
        "def find_best_model(base_path, model_type=\"best_clip\"):\n",
        "    \"\"\"\n",
        "    find the best saved model of a given type.\n",
        "\n",
        "    Args:\n",
        "        base_path: Base path where models are saved\n",
        "        model_type: Type of model to find (best_val, best_clip)\n",
        "\n",
        "    Returns:\n",
        "        Path to the best model, or None if no model found\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import glob\n",
        "\n",
        "    # look for exact match first\n",
        "    exact_path = f\"{base_path}_{model_type}.pt\"\n",
        "    if os.path.exists(exact_path):\n",
        "        return exact_path\n",
        "\n",
        "    # look for timestamped versions\n",
        "    pattern = f\"{base_path}_{model_type}_*.pt\"\n",
        "    matches = glob.glob(pattern)\n",
        "\n",
        "    if matches:\n",
        "        # sort by modification time (most recent first)\n",
        "        matches.sort(key=os.path.getmtime, reverse=True)\n",
        "        return matches[0]\n",
        "\n",
        "    # default to standard path\n",
        "    standard_path = f\"{base_path}.pt\"\n",
        "    if os.path.exists(standard_path):\n",
        "        return standard_path\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "u5l6LSDxo4h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# part 4: learning rate schedule\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, min_lr_ratio=0.1):\n",
        "    \"\"\"\n",
        "    create a schedule with a learning rate that decreases following the values of the\n",
        "    cosine function between the initial lr and 0, with a warmup period at the beginning\n",
        "\n",
        "    Args:\n",
        "        optimizer: optimizer for which to schedule the learning rate\n",
        "        num_warmup_steps: nr of steps for the warmup phase\n",
        "        num_training_steps: total nr of training steps\n",
        "        min_lr_ratio: min learning rate ratio compared to the initial LR\n",
        "\n",
        "    Returns:\n",
        "        learning rate scheduler\n",
        "    \"\"\"\n",
        "    def lr_lambda(current_step):\n",
        "        # warmup phase\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "\n",
        "        # cosine decay phase\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "        decayed = (1 - min_lr_ratio) * cosine_decay + min_lr_ratio\n",
        "\n",
        "        return decayed\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "def create_optimizer_and_scheduler(model_params, learning_rate, num_training_steps,\n",
        "                                  warmup_ratio=0.1, weight_decay=0.01):\n",
        "    \"\"\"\n",
        "    create an optimizer and learning rate scheduler with warmup.\n",
        "\n",
        "    Args:\n",
        "        model_params: parameters of the model to optimize\n",
        "        learning_rate: max learning rate\n",
        "        num_training_steps: total nr of training steps\n",
        "        warmup_ratio: portion of training to use for warmup\n",
        "        weight_decay: weight decay coefficient\n",
        "\n",
        "    Returns:\n",
        "        optimizer: AdamW optimizer\n",
        "        scheduler: learning rate scheduler\n",
        "    \"\"\"\n",
        "    # create optimizer with weight decay\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model_params,\n",
        "        lr=learning_rate,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    # create scheduler with warmup\n",
        "    num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    return optimizer, scheduler"
      ],
      "metadata": {
        "id": "Kq7HGF99pTT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 5: CLIP\n",
        "\n",
        "def compute_clip_reward_loss(features, img_ids, decoder, project_features,\n",
        "                         word2idx, idx2word, clip_calculator,\n",
        "                         reference_captions, image_paths, device):\n",
        "    \"\"\"\n",
        "    compute policy gradient loss using CLIP scores as rewards\n",
        "\n",
        "    Args:\n",
        "        features: batch of image features\n",
        "        img_ids: image IDs corresponding to the features\n",
        "        decoder: decoder model\n",
        "        project_features: projection layer\n",
        "        word2idx: word to index mapping\n",
        "        idx2word: index to word mapping\n",
        "        clip_calculator: CLIP calculator instance\n",
        "        reference_captions: dictionary of reference captions\n",
        "        image_paths: dictionary of image paths\n",
        "        device: device to run on\n",
        "\n",
        "    Returns:\n",
        "        loss: policy gradient loss\n",
        "        mean_clip_score: average CLIP score for the batch\n",
        "    \"\"\"\n",
        "    # sample captions and collect log probabilities\n",
        "    sampled_captions = {}\n",
        "    log_probs = []\n",
        "    valid_indices = []\n",
        "\n",
        "    # generate captions with sampling to allow gradient flow\n",
        "    for i, (feature, img_id) in enumerate(zip(features, img_ids)):\n",
        "        img_id = str(img_id)\n",
        "\n",
        "        # skip images without reference captions or paths\n",
        "        if img_id not in reference_captions or img_id not in image_paths:\n",
        "            continue\n",
        "\n",
        "        # sample a caption and get log probability\n",
        "        try:\n",
        "            _, log_prob, caption = sample_caption(\n",
        "                feature, decoder, project_features,\n",
        "                word2idx, idx2word, device\n",
        "            )\n",
        "\n",
        "            sampled_captions[img_id] = caption\n",
        "            log_probs.append(log_prob)\n",
        "            valid_indices.append(i)\n",
        "        except Exception as e:\n",
        "            print(f\"error sampling caption: {e}\")\n",
        "            continue\n",
        "\n",
        "    # if no valid captions, return zero loss\n",
        "    if not sampled_captions:\n",
        "        return torch.tensor(0.0, device=device), 0.0\n",
        "\n",
        "    # prepare data for CLIP evaluation\n",
        "    batch_references = {img_id: reference_captions[img_id]\n",
        "                       for img_id in sampled_captions.keys()}\n",
        "    batch_paths = {img_id: image_paths[img_id]\n",
        "                  for img_id in sampled_captions.keys()}\n",
        "\n",
        "    # calculate CLIP scores\n",
        "    try:\n",
        "        clip_scores = clip_calculator.compute(\n",
        "            batch_references, sampled_captions, batch_paths\n",
        "        )\n",
        "\n",
        "        rewards = clip_scores\n",
        "\n",
        "        # convert to tensor\n",
        "        rewards = torch.tensor(rewards, device=device)\n",
        "    except Exception as e:\n",
        "        print(f\"error calculating CLIP score: {e}\")\n",
        "        return torch.tensor(0.0, device=device), 0.0\n",
        "\n",
        "    # calculate policy gradient loss\n",
        "    # higher CLIP score ( better CLIP score) = Lower loss (hence the negative sign)\n",
        "    policy_loss = -rewards * torch.stack(log_probs).mean()\n",
        "\n",
        "    return policy_loss, clip_scores\n",
        "\n",
        "def evaluate_model_with_clip_score(decoder, project_features, eval_loader, word2idx, idx2word,\n",
        "                                  clip_calculator, tsv_path, image_dir,\n",
        "                                  train_loader, val_loader, max_eval_images=100, cached_refs=None, cached_paths=None):\n",
        "    \"\"\"\n",
        "    evaluate model using CLIP score (now with optimization for larger datasets).\n",
        "\n",
        "    Args:\n",
        "        decoder: decoder model\n",
        "        project_features: projection model\n",
        "        eval_loader: dataloader for evaluation\n",
        "        word2idx: word to index mapping\n",
        "        idx2word: index to word mapping\n",
        "        clip_calculator: CLIPCalculator instance\n",
        "        tsv_path: path to TSV file with reference captions\n",
        "        image_dir: directory with images\n",
        "        train_loader, val_loader: training/validation data loaders\n",
        "        max_eval_images: maximum number of images to evaluate (for speed)\n",
        "        cached_refs, cached_paths: optional cached references and paths\n",
        "\n",
        "    Returns:\n",
        "        clip_score: CLIP score\n",
        "        generated_captions: dict of generated captions\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    import random\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "    decoder.eval()\n",
        "    project_features.eval()\n",
        "\n",
        "    # get training and validation image IDs to ensure we only use images from our datasets\n",
        "    train_image_ids = set(train_loader.dataset.image_names)\n",
        "    val_image_ids = set(val_loader.dataset.image_names)\n",
        "    all_used_image_ids = train_image_ids.union(val_image_ids)\n",
        "\n",
        "    # load only necessary references (either from cache or from file)\n",
        "    if cached_refs is not None and cached_paths is not None:\n",
        "        reference_captions = cached_refs\n",
        "        image_paths = cached_paths\n",
        "        print(f\"using cached references with {len(reference_captions)} images\")\n",
        "    else:\n",
        "        # load references for all training/validation images\n",
        "        print(f\"loading reference captions for {len(all_used_image_ids)} training/validation images...\")\n",
        "        reference_captions, image_paths = load_references(tsv_path, image_dir, filter_ids=all_used_image_ids)\n",
        "        print(f\"loaded {len(reference_captions)} valid reference images with captions\")\n",
        "\n",
        "    # find images that have both features and reference captions\n",
        "    valid_image_ids = set(reference_captions.keys()).intersection(all_used_image_ids)\n",
        "    print(f\"found {len(valid_image_ids)} images with both features and reference captions\")\n",
        "\n",
        "    # limit to max_eval_images if needed (for faster evaluation)\n",
        "    if len(valid_image_ids) > max_eval_images:\n",
        "        print(f\"limiting CLIP evaluation to {max_eval_images} random images (out of {len(valid_image_ids)})\")\n",
        "        random.seed(42)  # for reproducibility\n",
        "        eval_image_ids = set(random.sample(list(valid_image_ids), max_eval_images))\n",
        "    else:\n",
        "        eval_image_ids = valid_image_ids\n",
        "        print(f\"evalauting all {len(eval_image_ids)} valid images with CLIP\")\n",
        "\n",
        "    # generate captions for each image in the evaluation set\n",
        "    generated_captions = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, img_ids in tqdm(eval_loader, desc=\"generating captions\"):\n",
        "            features = features.to(device)\n",
        "\n",
        "            for i, (feature, img_id) in enumerate(zip(features, img_ids)):\n",
        "                img_id = str(img_id.item() if hasattr(img_id, 'item') else img_id)\n",
        "\n",
        "                # skip if not in our evaluation set\n",
        "                if img_id not in eval_image_ids:\n",
        "                    continue\n",
        "\n",
        "                # double-check that we have a reference caption and image path\n",
        "                if img_id not in reference_captions or img_id not in image_paths:\n",
        "                    continue\n",
        "\n",
        "                img_feature = feature.unsqueeze(0)\n",
        "\n",
        "                # generate caption with beam search\n",
        "                caption = beam_search_caption(\n",
        "                    img_feature, decoder, project_features,\n",
        "                    word2idx, idx2word, device, beam_width=5, max_len=22\n",
        "                )\n",
        "\n",
        "                generated_captions[img_id] = caption\n",
        "\n",
        "    print(f\"generated {len(generated_captions)} captions\")\n",
        "\n",
        "    # final filtering to ensure all IDs are present in all dictionaries once and for all!! :(\n",
        "    common_ids = set(generated_captions.keys()).intersection(\n",
        "                    set(reference_captions.keys())).intersection(\n",
        "                    set(image_paths.keys()))\n",
        "\n",
        "    filtered_generated = {img_id: generated_captions[img_id] for img_id in common_ids}\n",
        "    filtered_reference = {img_id: reference_captions[img_id] for img_id in common_ids}\n",
        "    filtered_paths = {img_id: image_paths[img_id] for img_id in common_ids}\n",
        "\n",
        "    print(f\"final evaluation set: {len(common_ids)} images with all required data\")\n",
        "\n",
        "    if len(common_ids) == 0:\n",
        "        print(\"ERROR: no images have both generated captions, reference captions, and image paths!\")\n",
        "        return 0.0, {}\n",
        "\n",
        "    # calculate CLIP scores\n",
        "    print(\"computing CLIP scores...\")\n",
        "    generated_score = clip_calculator.compute(filtered_reference, filtered_generated, filtered_paths)\n",
        "    reference_score = clip_calculator.compute_reference_clip(filtered_reference, filtered_paths)\n",
        "\n",
        "    print(f\"CLIP scores - generated: {generated_score:.4f}, reference: {reference_score:.4f}\")\n",
        "\n",
        "    return generated_score, generated_captions\n"
      ],
      "metadata": {
        "id": "Rdqm537Ipv6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# part 6: training\n",
        "\n",
        "def train_in_phases(train_loader, val_loader, word2idx, idx2word, config):\n",
        "    \"\"\"\n",
        "    train a model in multiple phases with different objectives\n",
        "\n",
        "    Args:\n",
        "        train_loader: dataloader for training\n",
        "        val_loader: dataloader for validation\n",
        "        word2idx, idx2word: vocabulary mappings\n",
        "        config: base configuration for training\n",
        "\n",
        "    Returns:\n",
        "        decoder, project_features: trained models\n",
        "        histories: list of training histories for each phase\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import time\n",
        "    from datetime import datetime\n",
        "\n",
        "    # make a unique experiment directory\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    experiment_dir = os.path.join(BASE_PATH, f\"training_experiment_{timestamp}\")\n",
        "    os.makedirs(experiment_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Starting multi-phase training in {experiment_dir}\")\n",
        "\n",
        "    # save configuration\n",
        "    with open(os.path.join(experiment_dir, \"config.txt\"), \"w\") as f:\n",
        "        for key, value in config.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    # record histories for each phase\n",
        "    histories = []\n",
        "\n",
        "    # phase 1: initial training with cross-entropy loss only\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PHASE 1: Cross-Entropy Training\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    phase1_config = config.copy()\n",
        "    phase1_config.update({\n",
        "        'num_epochs': 10,\n",
        "        'clip_loss_weight': 0.0,  # no CLIP loss\n",
        "        'learning_rate': 0.0003,\n",
        "        'output_path': os.path.join(experiment_dir, \"phase1_model\")\n",
        "    })\n",
        "\n",
        "    # train with cross-entropy only\n",
        "    decoder, project_features, history1 = train_model_enhanced(\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        word2idx=word2idx,\n",
        "        idx2word=idx2word,\n",
        "        embed_size=phase1_config['embed_size'],\n",
        "        hidden_size=phase1_config['hidden_size'],\n",
        "        num_layers=phase1_config['num_layers'],\n",
        "        learning_rate=phase1_config['learning_rate'],\n",
        "        num_epochs=phase1_config['num_epochs'],\n",
        "        early_stopping_patience=phase1_config['early_stopping_patience'],\n",
        "        checkpoint_frequency=phase1_config['checkpoint_frequency'],\n",
        "        model_path=None,  # Start from scratch\n",
        "        output_path=phase1_config['output_path'],\n",
        "        feature_dim=phase1_config['feature_dim'],\n",
        "        clip_loss_weight=phase1_config['clip_loss_weight'],\n",
        "        clip_batch_size=phase1_config['clip_batch_size'],\n",
        "        clip_eval_frequency=phase1_config['clip_eval_frequency']\n",
        "    )\n",
        "\n",
        "    histories.append(history1)\n",
        "\n",
        "    # save phase 1 training curves\n",
        "    plot_training_curves(history1)\n",
        "    plt.savefig(os.path.join(experiment_dir, \"phase1_curves.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # find best model from phase 1\n",
        "    phase1_best_model = find_best_model(phase1_config['output_path'], \"best_val\")\n",
        "\n",
        "    # phase 2: balanced training with both losses\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PHASE 2: Combined Cross-Entropy and CLIP Training\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    phase2_config = config.copy()\n",
        "    phase2_config.update({\n",
        "        'num_epochs': 10,\n",
        "        'clip_loss_weight': 0.3,  # moderate CLIP influence\n",
        "        'learning_rate': 0.0001,  # lower learning rate for fine-tuning\n",
        "        'output_path': os.path.join(experiment_dir, \"phase2_model\")\n",
        "    })\n",
        "\n",
        "    # train with combined loss\n",
        "    decoder, project_features, history2 = train_model_enhanced(\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        word2idx=word2idx,\n",
        "        idx2word=idx2word,\n",
        "        embed_size=phase2_config['embed_size'],\n",
        "        hidden_size=phase2_config['hidden_size'],\n",
        "        num_layers=phase2_config['num_layers'],\n",
        "        learning_rate=phase2_config['learning_rate'],\n",
        "        num_epochs=phase2_config['num_epochs'],\n",
        "        early_stopping_patience=phase2_config['early_stopping_patience'],\n",
        "        checkpoint_frequency=phase2_config['checkpoint_frequency'],\n",
        "        model_path=phase1_best_model,  # Continue from phase 1\n",
        "        output_path=phase2_config['output_path'],\n",
        "        feature_dim=phase2_config['feature_dim'],\n",
        "        clip_loss_weight=phase2_config['clip_loss_weight'],\n",
        "        clip_batch_size=phase2_config['clip_batch_size'],\n",
        "        clip_eval_frequency=phase2_config['clip_eval_frequency']\n",
        "    )\n",
        "\n",
        "    histories.append(history2)\n",
        "\n",
        "    # save phase 2 training curves\n",
        "    plot_training_curves(history2)\n",
        "    plt.savefig(os.path.join(experiment_dir, \"phase2_curves.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # find best model from phase 2\n",
        "    phase2_best_model = find_best_model(phase2_config['output_path'], \"best_clip\")\n",
        "\n",
        "    # phase 3: CLIP fine-tuning\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PHASE 3: CLIP-Only Fine-Tuning\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    phase3_config = config.copy()\n",
        "    phase3_config.update({\n",
        "        'num_epochs': 8,\n",
        "        'clip_loss_weight': 0.9,  # high CLIP influence\n",
        "        'learning_rate': 5e-5,  # very low learning rate for fine-tuning\n",
        "        'output_path': os.path.join(experiment_dir, \"phase3_model\")\n",
        "    })\n",
        "\n",
        "    # train with high CLIP weight\n",
        "    decoder, project_features, history3 = train_model_enhanced(\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        word2idx=word2idx,\n",
        "        idx2word=idx2word,\n",
        "        embed_size=phase3_config['embed_size'],\n",
        "        hidden_size=phase3_config['hidden_size'],\n",
        "        num_layers=phase3_config['num_layers'],\n",
        "        learning_rate=phase3_config['learning_rate'],\n",
        "        num_epochs=phase3_config['num_epochs'],\n",
        "        early_stopping_patience=phase3_config['early_stopping_patience'],\n",
        "        checkpoint_frequency=phase3_config['checkpoint_frequency'],\n",
        "        model_path=phase2_best_model,  # Continue from phase 2\n",
        "        output_path=phase3_config['output_path'],\n",
        "        feature_dim=phase3_config['feature_dim'],\n",
        "        clip_loss_weight=phase3_config['clip_loss_weight'],\n",
        "        clip_batch_size=phase3_config['clip_batch_size'],\n",
        "        clip_eval_frequency=phase3_config['clip_eval_frequency']\n",
        "    )\n",
        "\n",
        "    histories.append(history3)\n",
        "\n",
        "    # save phase 3 training curves\n",
        "    plot_training_curves(history3)\n",
        "    plt.savefig(os.path.join(experiment_dir, \"phase3_curves.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # find the final best model\n",
        "    final_best_model = find_best_model(phase3_config['output_path'], \"best_clip\")\n",
        "\n",
        "    # if no phase 3 model is better, use phase 2's best\n",
        "    if not final_best_model:\n",
        "        final_best_model = phase2_best_model\n",
        "\n",
        "    # load the best overall model\n",
        "    decoder, project_features, _ = load_model(\n",
        "        final_best_model,\n",
        "        phase3_config['embed_size'],\n",
        "        len(word2idx),\n",
        "        phase3_config['hidden_size'],\n",
        "        phase3_config['num_layers'],\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # final evaluation with CLIP score\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL EVALUATION!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    eval_dataset = CaptionEvaluationDataset(val_loader.dataset.features_dict)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "    clip_calculator = CLIPCalculator()\n",
        "    final_clip_score, _ = evaluate_model_with_clip_score(\n",
        "        decoder, project_features, eval_loader, word2idx, idx2word,\n",
        "        clip_calculator, CAPTIONS_FILE, IMAGE_FOLDER,\n",
        "        train_loader, val_loader, max_eval_images=100\n",
        "    )\n",
        "\n",
        "    print(f\"final CLIP Score: {final_clip_score:.4f}\")\n",
        "\n",
        "    # create and save a final comparison plot\n",
        "    plot_training_phases_comparison(histories, experiment_dir)\n",
        "\n",
        "    return decoder, project_features, histories\n",
        "\n",
        "def plot_training_phases_comparison(histories, output_dir):\n",
        "    \"\"\"\n",
        "    create a plot comparing metrics across training phases\n",
        "\n",
        "    Args:\n",
        "        histories: list of training histories for each phase\n",
        "        output_dir: directory to save the plot\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    plt.figure(figsize=(15, 12))\n",
        "\n",
        "    # plot 1: training loss across phases\n",
        "    ax1 = plt.subplot(2, 2, 1)\n",
        "    colors = ['b', 'g', 'r']\n",
        "\n",
        "    for i, history in enumerate(histories):\n",
        "        if 'train_loss' in history and history['train_loss']:\n",
        "            # create epoch numbers for this phase\n",
        "            epochs = np.arange(len(history['train_loss']))\n",
        "\n",
        "            # offset epochs for phases after the first\n",
        "            if i > 0:\n",
        "                offset = sum(len(h['train_loss']) for h in histories[:i])\n",
        "                epochs = epochs + offset\n",
        "\n",
        "            ax1.plot(epochs, history['train_loss'], f'{colors[i]}-',\n",
        "                    label=f'Phase {i+1} Training Loss')\n",
        "\n",
        "    ax1.set_title('Training Loss Across Phases')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # plot 2: validation loss across phases\n",
        "    ax2 = plt.subplot(2, 2, 2)\n",
        "\n",
        "    for i, history in enumerate(histories):\n",
        "        if 'val_loss' in history and history['val_loss']:\n",
        "            # create epoch numbers for this phase\n",
        "            epochs = np.arange(len(history['val_loss']))\n",
        "\n",
        "            # offset epochs for phases after the first\n",
        "            if i > 0:\n",
        "                offset = sum(len(h['val_loss']) for h in histories[:i])\n",
        "                epochs = epochs + offset\n",
        "\n",
        "            ax2.plot(epochs, history['val_loss'], f'{colors[i]}-',\n",
        "                    label=f'Phase {i+1} Validation Loss')\n",
        "\n",
        "    ax2.set_title('Validation Loss Across Phases')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # plot 3: CLIP scores across phases\n",
        "    ax3 = plt.subplot(2, 2, 3)\n",
        "\n",
        "    for i, history in enumerate(histories):\n",
        "        if 'clip_scores' in history and history['clip_scores'] and 'eval_epochs' in history:\n",
        "            # get evaluation epochs for this phase\n",
        "            eval_epochs = history['eval_epochs']\n",
        "\n",
        "            # Offset epochs for phases after the first\n",
        "            if i > 0:\n",
        "                offset = sum(len(h['train_loss']) for h in histories[:i])\n",
        "                eval_epochs = [e + offset for e in eval_epochs]\n",
        "\n",
        "            ax3.plot(eval_epochs, history['clip_scores'], f'{colors[i]}-o',\n",
        "                    label=f'Phase {i+1} CLIP Score')\n",
        "\n",
        "    ax3.set_title('CLIP Scores Across Phases')\n",
        "    ax3.set_xlabel('Epoch')\n",
        "    ax3.set_ylabel('CLIP Score')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "    # plot 4: learning rates across phases\n",
        "    ax4 = plt.subplot(2, 2, 4)\n",
        "\n",
        "    for i, history in enumerate(histories):\n",
        "        if 'learning_rates' in history and history['learning_rates']:\n",
        "            steps = np.arange(len(history['learning_rates']))\n",
        "\n",
        "\n",
        "            if i > 0:\n",
        "                offset = sum(len(h['learning_rates']) for h in histories[:i])\n",
        "                steps = steps + offset\n",
        "\n",
        "            ax4.plot(steps, history['learning_rates'], f'{colors[i]}-',\n",
        "                    label=f'Phase {i+1} Learning Rate')\n",
        "\n",
        "    ax4.set_title('Learning Rate Schedule Across Phases')\n",
        "    ax4.set_xlabel('Training Step')\n",
        "    ax4.set_ylabel('Learning Rate')\n",
        "    ax4.set_yscale('log')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'all_phases_comparison.png'), dpi=150)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "eGa1IuF-ql2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# part 7: data preperation\n",
        "\n",
        "def extract_and_cache_features_threadpool(image_folder, captions_dict, cache_path=None, force_reload=False, num_workers=4, batch_size=64):\n",
        "    \"\"\"\n",
        "    extract image features using ResNet18 with ThreadPoolExecutor and cache them to disk.\n",
        "    this makes sure we don't have to extract the image features every time we want to start training.\n",
        "    and also its faster becaue i have to pay for colab aaa.\n",
        "\n",
        "    tried to optimize it for Google Colab T4 GPUs.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import torch\n",
        "    import torchvision.models as models\n",
        "    import torchvision.transforms as transforms\n",
        "    from PIL import Image\n",
        "    from tqdm.notebook import tqdm\n",
        "    import time\n",
        "    import concurrent.futures\n",
        "    import threading\n",
        "\n",
        "    # set default cache path if not provided\n",
        "    if cache_path is None:\n",
        "        cache_path = os.path.join(BASE_PATH, \"cached_features.pt\")\n",
        "\n",
        "    # check if cache exists and load it\n",
        "    all_features_dict = {}\n",
        "    if not force_reload and os.path.exists(cache_path):\n",
        "        print(f\"loading cached features from {cache_path}...\")\n",
        "        try:\n",
        "            all_features_dict = torch.load(cache_path)\n",
        "            print(f\"loaded features for {len(all_features_dict)} images from cache\")\n",
        "        except Exception as e:\n",
        "            print(f\"error loading cache: {e} :( \")\n",
        "            all_features_dict = {}\n",
        "\n",
        "    # create a new features_dict that only contains images from captions_dict\n",
        "    print(\"synchronizing feature and caption dictionaries...\")\n",
        "    features_dict = {}\n",
        "    missing_images = []\n",
        "\n",
        "    # first, add features from cache for images in captions_dict\n",
        "    for img_id in captions_dict.keys():\n",
        "        if img_id in all_features_dict:\n",
        "            features_dict[img_id] = all_features_dict[img_id]\n",
        "        else:\n",
        "            missing_images.append(img_id)\n",
        "\n",
        "    print(f\"using {len(features_dict)} cached features\")\n",
        "    print(f\"need to extract features for {len(missing_images)} more images\")\n",
        "\n",
        "    # if we already have all features we need, return them\n",
        "    if not missing_images:\n",
        "        print(\"All required features found in cache!\")\n",
        "        return features_dict\n",
        "\n",
        "    # otherwise, we need to extract features for missing images\n",
        "    # load resnet18 and remove final classification layer\n",
        "    print(\"loading resnet 18 model...\")\n",
        "    resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "    resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
        "    resnet.eval().to(device)\n",
        "\n",
        "    # image transformation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                          std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    def load_image(img_name):\n",
        "        \"\"\"load and preprocess a single image\"\"\"\n",
        "        img_path = os.path.join(image_folder, f\"{img_name}.jpg\")\n",
        "        if not os.path.exists(img_path):\n",
        "            return None, None\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            img_tensor = transform(image)\n",
        "            return img_name, img_tensor\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_name}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    # start extraction timer\n",
        "    print(f\"extracting features for {len(missing_images)} images using {num_workers} threads and batch size {batch_size}...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # result dictionary and lock for thread safety\n",
        "    result_lock = threading.Lock()\n",
        "\n",
        "    # process images in batches\n",
        "    extracted_count = 0\n",
        "    with tqdm(total=len(missing_images)) as pbar:\n",
        "        # process all images in batches\n",
        "        for i in range(0, len(missing_images), batch_size):\n",
        "            batch_names = missing_images[i:i + batch_size]\n",
        "\n",
        "            # load images in parallel\n",
        "            loaded_images = []\n",
        "            with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "                for result in executor.map(load_image, batch_names):\n",
        "                    if result[0] is not None:  # skip failed images\n",
        "                        loaded_images.append(result)\n",
        "\n",
        "            # skip empty batches\n",
        "            if not loaded_images:\n",
        "                pbar.update(len(batch_names))\n",
        "                continue\n",
        "\n",
        "            # separate names and tensors\n",
        "            batch_img_names = [img[0] for img in loaded_images]\n",
        "            batch_tensors = [img[1] for img in loaded_images]\n",
        "\n",
        "            # process batch on GPU\n",
        "            try:\n",
        "                # stack tensors and move to GPU\n",
        "                stacked_tensors = torch.stack(batch_tensors).to(device)\n",
        "\n",
        "                # extract features in a single forward pass\n",
        "                with torch.no_grad():\n",
        "                    features = resnet(stacked_tensors).squeeze(-1).squeeze(-1).cpu()\n",
        "\n",
        "                # save features to dictionanry\n",
        "                with result_lock:\n",
        "                    for j, img_name in enumerate(batch_img_names):\n",
        "                        features_dict[img_name] = features[j]\n",
        "                        all_features_dict[img_name] = features[j]  # update the full cache\n",
        "                        extracted_count += 1\n",
        "\n",
        "                    # save checkpoint every 1000 images (you live and you learn...)\n",
        "                    if extracted_count % 1000 == 0:\n",
        "                        print(f\"\\nsaving checkpoint after {extracted_count} images...\")\n",
        "                        torch.save(all_features_dict, cache_path + \".temp\")\n",
        "            except Exception as e:\n",
        "                print(f\"error processing batch: {e}\")\n",
        "\n",
        "            # update progress\n",
        "            pbar.update(len(batch_names))\n",
        "\n",
        "    # save the final features dictionary\n",
        "    print(f\"extracted features for {extracted_count} images in {time.time() - start_time:.2f} seconds\")\n",
        "    print(f\"ttotal features in cache: {len(all_features_dict)}\")\n",
        "    print(f\"features for this run: {len(features_dict)}\")\n",
        "    print(f\"saving features to {cache_path}...\")\n",
        "    torch.save(all_features_dict, cache_path)\n",
        "\n",
        "    # remove temporary file if it exists\n",
        "    if os.path.exists(cache_path + \".temp\"):\n",
        "        os.remove(cache_path + \".temp\")\n",
        "\n",
        "    return features_dict\n",
        "\n",
        "\n",
        "def prepare_data_with_cache(captions_file, image_folder, max_images=19000, cache_path=None, force_reload=False, num_workers=8, batch_size=64):\n",
        "    \"\"\"\n",
        "    prepare data for training, with feature caching support this time.\n",
        "    \"\"\"\n",
        "    print(\"loading captions...\")\n",
        "    # load and clean captions\n",
        "    df = pd.read_csv(captions_file, sep=\"\\t\", names=[\"image\", \"caption\"])\n",
        "    df = df.dropna()\n",
        "    df[\"image\"] = df[\"image\"].astype(str).str.strip()\n",
        "\n",
        "    # remove .jpg extension if still there\n",
        "    df[\"image\"] = df[\"image\"].apply(lambda x: x[:-4] if x.endswith('.jpg') else x)\n",
        "\n",
        "    # build dictionary: image -> [captions]\n",
        "    captions_dict = {}\n",
        "    for _, row in df.iterrows():\n",
        "        img_name = row[\"image\"]\n",
        "        caption = row[\"caption\"]\n",
        "        if img_name not in captions_dict:\n",
        "            captions_dict[img_name] = []\n",
        "        captions_dict[img_name].append(caption)\n",
        "\n",
        "    # limit to max_images if needed\n",
        "    if len(captions_dict) > max_images:\n",
        "        print(f\"limiting dataset from {len(captions_dict)} to {max_images} images\")\n",
        "        random.seed(42)\n",
        "        subset_ids = random.sample(list(captions_dict.keys()), max_images)\n",
        "        subset_captions_dict = {img_id: captions_dict[img_id] for img_id in subset_ids}\n",
        "        captions_dict = subset_captions_dict\n",
        "\n",
        "    print(f\"using {len(captions_dict)} images with captions\")\n",
        "\n",
        "    # make vocabulary\n",
        "    all_captions = sum(captions_dict.values(), [])\n",
        "    words = [word for caption in all_captions for word in caption.split()]\n",
        "    most_common = Counter(words).most_common(4900)\n",
        "\n",
        "    # special tokens first\n",
        "    vocab = [\"<PAD>\", \"< SOS >\", \"<EOS>\", \"<UNK>\"] + [w for w, _ in most_common]\n",
        "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "    idx2word = {i: w for w, i in word2idx.items()}\n",
        "    vocab_size = len(vocab)\n",
        "    print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "    # extract image features with caching\n",
        "    print(\"extracting image features (with multithreaded caching)...\")\n",
        "    features_dict = extract_and_cache_features_threadpool(\n",
        "        image_folder,\n",
        "        captions_dict,\n",
        "        cache_path=cache_path,\n",
        "        force_reload=force_reload,\n",
        "        num_workers=num_workers,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "\n",
        "    caption_keys = set(captions_dict.keys())\n",
        "    feature_keys = set(features_dict.keys())\n",
        "\n",
        "    print(f\"caption keys: {len(caption_keys)}, feature keys: {len(feature_keys)}\")\n",
        "\n",
        "    # find any discrepancies\n",
        "    missing_captions = feature_keys - caption_keys\n",
        "    missing_features = caption_keys - feature_keys\n",
        "\n",
        "    if missing_captions:\n",
        "        print(f\"WARNING: {len(missing_captions)} images have features but no captions\")\n",
        "        # remove them from features_dict\n",
        "        for img_id in missing_captions:\n",
        "            if img_id in features_dict:\n",
        "                del features_dict[img_id]\n",
        "\n",
        "    if missing_features:\n",
        "        print(f\"WARNING: {len(missing_features)} images have captions but no features\")\n",
        "        # remove them from captions_dict\n",
        "        for img_id in missing_features:\n",
        "            if img_id in captions_dict:\n",
        "                del captions_dict[img_id]\n",
        "\n",
        "    # final check\n",
        "    print(f\"final dataset: {len(features_dict)} images with both features and captions\")\n",
        "\n",
        "    return features_dict, captions_dict, word2idx, idx2word"
      ],
      "metadata": {
        "id": "ch7-8DP6ssxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_model_enhanced(\n",
        "        train_loader, val_loader, word2idx, idx2word,\n",
        "        embed_size=256, hidden_size=512, num_layers=4,\n",
        "        learning_rate=0.0003, num_epochs=20,\n",
        "        early_stopping_patience=5, checkpoint_frequency=1,\n",
        "        model_path=None, output_path=None, feature_dim=512,\n",
        "        clip_loss_weight=0.5, clip_batch_size=16,\n",
        "        clip_eval_frequency=50, warmup_ratio=0.1,\n",
        "        weight_decay=0.01):\n",
        "    \"\"\"\n",
        "    enhanced training function that integrates CLIP scores using policy gradients\n",
        "\n",
        "    Args:\n",
        "        train_loader: dataloader for training\n",
        "        val_loader: dataloader for validation\n",
        "        word2idx, idx2word: vocabulary mappings\n",
        "        embed_size, hidden_size, num_layers: model architecture parameters\n",
        "        learning_rate: learning rate\n",
        "        num_epochs: nr of epochs\n",
        "        early_stopping_patience: nr of epochs to wait before early stopping\n",
        "        checkpoint_frequency: how often to save checkpoints (in epochs)\n",
        "        model_path: path to load pre-trained model (if resuming training)\n",
        "        output_path: where to save models\n",
        "        feature_dim: dimension of image features (512 for ResNet18, 2048 for ResNet50)\n",
        "        clip_loss_weight: weight for the CLIP-based loss component (0-1)\n",
        "        clip_batch_size: nr of samples for CLIP evaluation in each batch\n",
        "        clip_eval_frequency: how often to evaluate CLIP loss (in training steps)\n",
        "        warmup_ratio: portion of training to use for warmup\n",
        "        weight_decay: weight decay coefficient\n",
        "\n",
        "    Returns:\n",
        "        trained decoder and project_features models, training history\n",
        "    \"\"\"\n",
        "    import time\n",
        "    from datetime import datetime, timedelta\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "    # initialize training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'clip_scores': [],\n",
        "        'clip_batch_scores': [],\n",
        "        'learning_rates': [],\n",
        "        'eval_epochs': [],\n",
        "        'best_val_loss': float('inf'),\n",
        "        'best_clip_score': 0.0,\n",
        "        'epochs_without_improvement': 0,\n",
        "        'start_time': time.time(),\n",
        "        'total_training_time': 0\n",
        "    }\n",
        "\n",
        "    vocab_size = len(word2idx)\n",
        "\n",
        "    # initialize CLIP calculator\n",
        "    clip_calculator = CLIPCalculator()\n",
        "\n",
        "    # cache reference captions and image paths\n",
        "    print(\"loading and caching reference captions...\")\n",
        "    train_image_ids = set(train_loader.dataset.image_names)\n",
        "    val_image_ids = set(val_loader.dataset.image_names)\n",
        "    all_used_image_ids = train_image_ids.union(val_image_ids)\n",
        "    cached_references, cached_image_paths = load_references(\n",
        "        CAPTIONS_FILE, IMAGE_FOLDER, filter_ids=all_used_image_ids)\n",
        "    print(f\"cached {len(cached_references)} reference cations\")\n",
        "\n",
        "    # initialize or load modsels\n",
        "    if model_path:\n",
        "        try:\n",
        "            decoder, project_features, metadata = load_model(\n",
        "                model_path, embed_size, vocab_size, hidden_size, num_layers, device\n",
        "            )\n",
        "\n",
        "            # check if feature dimensions match\n",
        "            if project_features.weight.size(1) != feature_dim:\n",
        "                print(f\"warning: model expects {project_features.weight.size(1)}-dim features, but we have {feature_dim}-dim features\")\n",
        "                print(\"re-initializing projection layer...\")\n",
        "                project_features = nn.Linear(feature_dim, embed_size).to(device)\n",
        "\n",
        "            # Load training history if available\n",
        "            if 'metrics' in metadata and isinstance(metadata['metrics'], dict):\n",
        "                for key in history:\n",
        "                    if key in metadata['metrics']:\n",
        "                        history[key] = metadata['metrics'][key]\n",
        "                print(f\"continue training from epoch {len(history['train_loss'])+1}\")\n",
        "        except Exception as e:\n",
        "            print(f\"error loading model: {e}\")\n",
        "            print(\"initializing new model...\")\n",
        "            decoder = Transformer_Decoder(embed_size, vocab_size, hidden_size, num_layers).to(device)\n",
        "            project_features = nn.Linear(feature_dim, embed_size).to(device)\n",
        "    else:\n",
        "        # initialize new models\n",
        "        decoder = Transformer_Decoder(embed_size, vocab_size, hidden_size, num_layers).to(device)\n",
        "        project_features = nn.Linear(feature_dim, embed_size).to(device)\n",
        "        print(\"initialized new model\")\n",
        "\n",
        "    # create optimizer and scheduler\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    optimizer, scheduler = create_optimizer_and_scheduler(\n",
        "        list(decoder.parameters()) + list(project_features.parameters()),\n",
        "        learning_rate=learning_rate,\n",
        "        num_training_steps=total_steps,\n",
        "        warmup_ratio=warmup_ratio,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    # cross-entropy loss for supervised training\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<PAD>\"])\n",
        "\n",
        "    # evaluation dataset for CLIP scoring\n",
        "    eval_dataset = CaptionEvaluationDataset(val_loader.dataset.features_dict)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"starting training with {num_epochs} epochs\")\n",
        "    print(f\"feature dimension: {feature_dim}\")\n",
        "    print(f\"architecture: {num_layers} layers, {hidden_size} hidden dim, {embed_size} embed dim\")\n",
        "    print(f\"CLIP loss weight: {clip_loss_weight}\")\n",
        "    print(f\"evaluating CLIP every {clip_eval_frequency} steps\")\n",
        "\n",
        "    start_epoch = len(history['train_loss'])\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # training phase\n",
        "        decoder.train()\n",
        "        project_features.train()\n",
        "        total_ce_loss = 0\n",
        "        total_clip_loss = 0\n",
        "        total_combined_loss = 0\n",
        "        epoch_clip_scores = []\n",
        "        num_clip_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"epoch {epoch+1}/{start_epoch+num_epochs} [Train]\")\n",
        "        for batch_idx, (features, captions) in enumerate(progress_bar):\n",
        "            features, captions = features.to(device), captions.to(device)\n",
        "\n",
        "            # standard cross-entropy loss\n",
        "            optimizer.zero_grad()\n",
        "            projected = project_features(features)\n",
        "            output = decoder(projected, captions[:, :-1])\n",
        "            ce_loss = criterion(output.reshape(-1, vocab_size), captions[:, 1:].reshape(-1))\n",
        "\n",
        "            # determine whether to compute CLIP reward loss\n",
        "            compute_clip = (global_step % clip_eval_frequency == 0) and (clip_loss_weight > 0)\n",
        "            clip_loss = torch.tensor(0.0, device=device)\n",
        "            clip_score = 0.0\n",
        "\n",
        "            # compute policy gradient loss with CLIP rewards (occasionally)\n",
        "            if compute_clip:\n",
        "                # select a subset of images for CLIP evaluation\n",
        "                clip_eval_indices = random.sample(\n",
        "                    range(len(features)), min(clip_batch_size, len(features)))\n",
        "\n",
        "                clip_features = features[clip_eval_indices]\n",
        "                img_ids = [train_loader.dataset.image_names[batch_idx * train_loader.batch_size + i]\n",
        "                          for i in clip_eval_indices]\n",
        "\n",
        "                # calculate policy gradient loss with CLIP as reward\n",
        "                clip_loss, clip_score = compute_clip_reward_loss(\n",
        "                    clip_features, img_ids, decoder, project_features,\n",
        "                    word2idx, idx2word, clip_calculator,\n",
        "                    cached_references, cached_image_paths, device\n",
        "                )\n",
        "\n",
        "                if clip_score > 0:\n",
        "                    epoch_clip_scores.append(clip_score)\n",
        "                    num_clip_batches += 1\n",
        "\n",
        "            # combine losses\n",
        "            if compute_clip and clip_score > 0:\n",
        "                combined_loss = (1 - clip_loss_weight) * ce_loss + clip_loss_weight * clip_loss\n",
        "                total_clip_loss += clip_loss.item()\n",
        "            else:\n",
        "                combined_loss = ce_loss\n",
        "\n",
        "            # backprop and optimization\n",
        "            combined_loss.backward()\n",
        "\n",
        "            # gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(project_features.parameters(), max_norm=1.0)\n",
        "\n",
        "            # update weights\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # track current learning rate\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "            # update progress bar and tracking variables\n",
        "            total_ce_loss += ce_loss.item()\n",
        "            total_combined_loss += combined_loss.item()\n",
        "\n",
        "            if compute_clip and clip_score > 0:\n",
        "                progress_bar.set_postfix(\n",
        "                    CE_Loss=f\"{ce_loss.item():.4f}\",\n",
        "                    CLIP_Loss=f\"{clip_loss.item():.4f}\",\n",
        "                    CLIP_Score=f\"{clip_score:.4f}\",\n",
        "                    LR=f\"{current_lr:.6f}\"\n",
        "                )\n",
        "            else:\n",
        "                progress_bar.set_postfix(\n",
        "                    CE_Loss=f\"{ce_loss.item():.4f}\",\n",
        "                    LR=f\"{current_lr:.6f}\"\n",
        "                )\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        # calculate average losses for the epoch\n",
        "        avg_ce_loss = total_ce_loss / len(train_loader)\n",
        "        avg_combined_loss = total_combined_loss / len(train_loader)\n",
        "\n",
        "        # calculate average CLIP score if available\n",
        "        avg_clip_score = sum(epoch_clip_scores) / num_clip_batches if num_clip_batches > 0 else 0.0\n",
        "\n",
        "        # update the history\n",
        "        history['train_loss'].append(avg_ce_loss)\n",
        "        history['clip_batch_scores'].append(avg_clip_score)\n",
        "        history['learning_rates'].append(current_lr)\n",
        "\n",
        "        # validation phase\n",
        "        decoder.eval()\n",
        "        project_features.eval()\n",
        "        total_val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for features, captions in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{start_epoch+num_epochs} [Val]\"):\n",
        "                features, captions = features.to(device), captions.to(device)\n",
        "                projected = project_features(features)\n",
        "                output = decoder(projected, captions[:, :-1])\n",
        "                loss = criterion(output.reshape(-1, vocab_size), captions[:, 1:].reshape(-1))\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "        # calculate epoch time\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        total_time = time.time() - history['start_time']\n",
        "        history['total_training_time'] = total_time\n",
        "\n",
        "        # print progress summary\n",
        "        print(f\"epoch {epoch+1}/{start_epoch+num_epochs} - \"\n",
        "              f\"train CE Loss: {avg_ce_loss:.4f}, \"\n",
        "              f\"val Loss: {avg_val_loss:.4f}, \"\n",
        "              f\"CLIP Score: {avg_clip_score:.4f}, \"\n",
        "              f\"time: {epoch_time:.1f}s, \"\n",
        "              f\"total: {timedelta(seconds=int(total_time))}\")\n",
        "\n",
        "        # save checkpoint if needed\n",
        "        if (epoch + 1) % checkpoint_frequency == 0:\n",
        "            save_model(\n",
        "                decoder, project_features, history, output_path,\n",
        "                model_type=f\"checkpoint\"\n",
        "            )\n",
        "\n",
        "        # track best validation loss\n",
        "        if avg_val_loss < history['best_val_loss']:\n",
        "            history['best_val_loss'] = avg_val_loss\n",
        "            history['epochs_without_improvement'] = 0\n",
        "            save_model(\n",
        "                decoder, project_features, history, output_path,\n",
        "                model_type=\"best_val\"\n",
        "            )\n",
        "            print(f\"new best model saved based on validation loss: {avg_val_loss:.4f}\")\n",
        "        else:\n",
        "            history['epochs_without_improvement'] += 1\n",
        "\n",
        "        # Evaluate with CLIP score every few epochs or at the end\n",
        "        evaluate_clip = ((epoch + 1) % 2 == 0) or (epoch == start_epoch + num_epochs - 1)\n",
        "\n",
        "        if evaluate_clip:\n",
        "            print(\"evaluating with CLIP score...\")\n",
        "            clip_score, generated_captions = evaluate_model_with_clip_score(\n",
        "                decoder, project_features, eval_loader, word2idx, idx2word,\n",
        "                clip_calculator, CAPTIONS_FILE, IMAGE_FOLDER,\n",
        "                train_loader, val_loader, max_eval_images=100,\n",
        "                cached_refs=cached_references, cached_paths=cached_image_paths\n",
        "            )\n",
        "\n",
        "            history['clip_scores'].append(clip_score)\n",
        "            history['eval_epochs'].append(epoch)\n",
        "\n",
        "            # Save model if CLIP score improved\n",
        "            if clip_score > history['best_clip_score']:\n",
        "                history['best_clip_score'] = clip_score\n",
        "                save_model(\n",
        "                    decoder, project_features, history, output_path,\n",
        "                    model_type=\"best_clip\"\n",
        "                )\n",
        "                print(f\"new best model saved based on CLIP score: {clip_score:.4f}\")\n",
        "\n",
        "        # Sample some captions every few epochs\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            print(\"\\nsample captions:\")\n",
        "            sample_features, _ = next(iter(val_loader))\n",
        "            for i in range(min(3, len(sample_features))):\n",
        "                feature = sample_features[i].unsqueeze(0).to(device)\n",
        "                caption = beam_search_caption(\n",
        "                    feature, decoder, project_features,\n",
        "                    word2idx, idx2word, device, beam_width=3\n",
        "                )\n",
        "                print(f\"sampel {i+1}: {caption}\")\n",
        "            print()\n",
        "\n",
        "        # Early stopping check\n",
        "        if history['epochs_without_improvement'] >= early_stopping_patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    # Plot training curves\n",
        "    plot_training_curves(history)\n",
        "\n",
        "    # Load the best model (using CLIP score)\n",
        "    best_model_path = find_best_model(output_path, \"best_clip\")\n",
        "    if best_model_path:\n",
        "        print(f\"loading best model from {best_model_path}\")\n",
        "        decoder, project_features, _ = load_model(\n",
        "            best_model_path, embed_size, vocab_size, hidden_size, num_layers, device\n",
        "        )\n",
        "\n",
        "    return decoder, project_features, history\n",
        "\n",
        "def plot_training_curves(history):\n",
        "    \"\"\"\n",
        "    plot enhanced training curves with more metrics\n",
        "\n",
        "    Args:\n",
        "        history: training history dictionary\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # multi-panel figure\n",
        "    fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "    # 1: loss curves\n",
        "    ax1 = fig.add_subplot(2, 2, 1)\n",
        "    ax1.plot(history['train_loss'], 'b-', label='Training CE Loss')\n",
        "    ax1.plot(history['val_loss'], 'r-', label='Validation Loss')\n",
        "\n",
        "    if 'best_val_loss' in history:\n",
        "        ax1.axhline(y=history['best_val_loss'], color='r', linestyle='--',\n",
        "                   label=f'Best Val Loss: {history[\"best_val_loss\"]:.4f}')\n",
        "\n",
        "    ax1.set_title('Loss Curves')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # 2: CLIP scores\n",
        "    ax2 = fig.add_subplot(2, 2, 2)\n",
        "\n",
        "    # plot CLIP batch scores if available\n",
        "    if 'clip_batch_scores' in history and any(history['clip_batch_scores']):\n",
        "        clip_batch_x = list(range(len(history['clip_batch_scores'])))\n",
        "        ax2.plot(clip_batch_x, history['clip_batch_scores'], 'g-', alpha=0.5,\n",
        "                label='Training CLIP Scores')\n",
        "\n",
        "    # lot evaluation CLIP scores if available\n",
        "    if 'clip_scores' in history and 'eval_epochs' in history and history['clip_scores']:\n",
        "        ax2.plot(history['eval_epochs'], history['clip_scores'], 'g-o',\n",
        "                label='Evaluation CLIP Scores')\n",
        "\n",
        "        if 'best_clip_score' in history:\n",
        "            ax2.axhline(y=history['best_clip_score'], color='r', linestyle='--',\n",
        "                       label=f'Best CLIP Score: {history[\"best_clip_score\"]:.4f}')\n",
        "\n",
        "    ax2.set_title('CLIP Score Progression')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('CLIP Score')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # 3: lr\n",
        "    ax3 = fig.add_subplot(2, 2, 3)\n",
        "    if 'learning_rates' in history and history['learning_rates']:\n",
        "        ax3.plot(history['learning_rates'], 'c-')\n",
        "        ax3.set_title('Learning Rate Schedule')\n",
        "        ax3.set_xlabel('Step')\n",
        "        ax3.set_ylabel('Learning Rate')\n",
        "        ax3.set_yscale('log')  # Log scale for better visualization\n",
        "        ax3.grid(True)\n",
        "\n",
        "    # 4: combined metrics (optional)\n",
        "    ax4 = fig.add_subplot(2, 2, 4)\n",
        "\n",
        "    # secondary axis for CLIP score\n",
        "    if ('clip_scores' in history and history['clip_scores'] and\n",
        "        'eval_epochs' in history and 'val_loss' in history):\n",
        "\n",
        "        # plot validation loss on primary axis\n",
        "        epochs = list(range(len(history['val_loss'])))\n",
        "        line1 = ax4.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
        "        ax4.set_xlabel('Epoch')\n",
        "        ax4.set_ylabel('Validation Loss', color='r')\n",
        "        ax4.tick_params(axis='y', labelcolor='r')\n",
        "\n",
        "        # secondary axis for CLIP score\n",
        "        ax4_twin = ax4.twinx()\n",
        "        line2 = ax4_twin.plot(history['eval_epochs'], history['clip_scores'], 'g-o',\n",
        "                             label='CLIP Score')\n",
        "        ax4_twin.set_ylabel('CLIP Score', color='g')\n",
        "        ax4_twin.tick_params(axis='y', labelcolor='g')\n",
        "\n",
        "        # combine legends\n",
        "        lines = line1 + line2\n",
        "        labels = [l.get_label() for l in lines]\n",
        "        ax4.legend(lines, labels, loc='upper right')\n",
        "\n",
        "        ax4.set_title('Validation Loss vs CLIP Score')\n",
        "        ax4.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{BASE_PATH}/enhanced_training_curves.png\", dpi=150)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "_Ed65AubtiZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline():\n",
        "    \"\"\"\n",
        "    main execution pipeline for the improved image captioning fine-tuning\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import torch\n",
        "    import random\n",
        "    import numpy as np\n",
        "    from torch.utils.data import DataLoader\n",
        "    from datetime import datetime\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"starting image captioning fine-tuning pipeline at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"using: {device}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # set random seeds for reproducibility\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "\n",
        "    # base configuration\n",
        "    base_config = {\n",
        "        'captions_file': CAPTIONS_FILE,\n",
        "        'image_folder': f\"{BASE_PATH}/images/loadedimages\",\n",
        "        'max_images': 9000,\n",
        "        'batch_size': 64,\n",
        "        'embed_size': 256,\n",
        "        'hidden_size': 768,\n",
        "        'num_layers': 6,\n",
        "        'learning_rate': 0.0003,\n",
        "        'num_epochs': 20,\n",
        "        'early_stopping_patience': 5,\n",
        "        'checkpoint_frequency': 2,\n",
        "        'num_workers': 4,\n",
        "        'feature_extraction_workers': 96,\n",
        "        'feature_extraction_batch_size': 128,\n",
        "        'clip_loss_weight': 0.3,\n",
        "        'clip_batch_size': 16,\n",
        "        'clip_eval_frequency': 100,  # evaluate less frequently to speed up training\n",
        "        'seed': 42,\n",
        "        'feature_dim': 512,  # specific for resnet18!!\n",
        "        'feature_cache_path': os.path.join(BASE_PATH, \"cached_features.pt\"),\n",
        "        'force_reload_features': False,\n",
        "    }\n",
        "\n",
        "    print(\"\\nConfiguration:\")\n",
        "    for key, value in base_config.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    # prepare data\n",
        "    print(\"\\n preparing data...\")\n",
        "    features_dict, captions_dict, word2idx, idx2word = prepare_data_with_cache(\n",
        "        base_config['captions_file'],\n",
        "        base_config['image_folder'],\n",
        "        max_images=base_config['max_images'],\n",
        "        cache_path=base_config['feature_cache_path'],\n",
        "        force_reload=base_config['force_reload_features'],\n",
        "        num_workers=base_config['feature_extraction_workers'],\n",
        "        batch_size=base_config['feature_extraction_batch_size']\n",
        "    )\n",
        "\n",
        "    # verify feature dimension\n",
        "    sample_feature = next(iter(features_dict.values()))\n",
        "    actual_feature_dim = sample_feature.size(0)\n",
        "    print(f\"detected feature dimension: {actual_feature_dim}\")\n",
        "    if actual_feature_dim != base_config['feature_dim']:\n",
        "        print(f\"updating feature dimension from {base_config['feature_dim']} to {actual_feature_dim}\")\n",
        "        base_config['feature_dim'] = actual_feature_dim\n",
        "\n",
        "    # create datasets\n",
        "    print(\"\\n creating datasets...\")\n",
        "    train_dataset = CaptionFeatureDataset(\n",
        "        features_dict, captions_dict, word2idx, max_len=22, split='train'\n",
        "    )\n",
        "    val_dataset = CaptionFeatureDataset(\n",
        "        features_dict, captions_dict, word2idx, max_len=22, split='val'\n",
        "    )\n",
        "\n",
        "    # create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=base_config['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=base_config['num_workers'],\n",
        "        pin_memory=True if device == \"cuda\" else False\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=base_config['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=base_config['num_workers'],\n",
        "        pin_memory=True if device == \"cuda\" else False\n",
        "    )\n",
        "\n",
        "    print(f\"training set: {len(train_dataset)} samples\")\n",
        "    print(f\"validation set: {len(val_dataset)} samples\")\n",
        "\n",
        "    # train in phases\n",
        "    print(\"\\n running multi-phase training...\")\n",
        "    decoder, project_features, histories = train_in_phases(\n",
        "        train_loader, val_loader, word2idx, idx2word, base_config\n",
        "    )\n",
        "\n",
        "    # final evaluation\n",
        "    print(\"\\n performing final evaluation...\")\n",
        "    # make evaluation dataset\n",
        "    eval_dataset = CaptionEvaluationDataset(val_loader.dataset.features_dict)\n",
        "    eval_loader = DataLoader(\n",
        "        eval_dataset, batch_size=16, shuffle=False, num_workers=2\n",
        "    )\n",
        "\n",
        "    # evaluate with CLIP score\n",
        "    clip_calculator = CLIPCalculator()\n",
        "    final_clip_score, generated_captions = evaluate_model_with_clip_score(\n",
        "        decoder, project_features, eval_loader, word2idx, idx2word,\n",
        "        clip_calculator, CAPTIONS_FILE, IMAGE_FOLDER,\n",
        "        train_loader, val_loader, max_eval_images=100\n",
        "    )\n",
        "\n",
        "    print(f\"\\n final CLIP Score: {final_clip_score:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"image captioning fine-tuning pipeline complete!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return decoder, project_features, histories\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()"
      ],
      "metadata": {
        "id": "MeZB2eB-nval"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}